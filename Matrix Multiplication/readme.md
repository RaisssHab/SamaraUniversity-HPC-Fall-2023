<h1>Matrix Multiplication</h1>
<h2>Постановка цели эксперимента</h2>

Цель – исследовать ускорение при произведении матриц на CUDA относительно последовательной реализации.

Задачи:
1.	Рассчитать время, достигаемое последовательной реализацией алгоритма.
2.	Рассчитать время и ускорение, достигаемое с использованием написанной параллельной реализации на CUDA.
3.	Проанализировать результат, сделать выводы.

<h2>Инструментальные средства эксперимента</h2>
<h3>Программные средства</h3>
Язык программирования – C++. 
Последовательная реализация:

```
void sequential_mult(const double* A, const double* B, double* C, const int N, const int M, const int K)
{
    for (int i = 0; i < N; ++i) {
        for (int j = 0; j < K; ++j) {
            C[IDX2C(i, j, N)] = 0;
        }
    }

    for (int i = 0; i < N; ++i) {
        for (int j = 0; j < K; ++j) {
            int ij_idx = IDX2C(i, j, N);
            for (int k = 0; k < M; ++k) {
                const double& c1 = A[IDX2C(i, k, N)];
                const double& c2 = B[IDX2C(k, j, M)];
                C[ij_idx] += c1 * c2;
            }
        }
    }
}
```

Параллельная реализация:

```
__global__ void addKernel(double* C, double* A, double* B, const int N, const int M, const int K)
{
    // введем переменные i, j  которые характеризуют позицию нити в гриде
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    int j = blockIdx.y * blockDim.y + threadIdx.y;

    // поскольку размерность грида, выраженная в нитях, необязательно совпадет с размерностью результирующей матрицы, следует проверять наличиие выхода за пределы матрицы
    if (i >= N || j >= K) {
        return;
    }

    // выполняем третий по вложенности цикл при последовательном произведении - скалярное произведение
    int ij_idx = IDX2C(i, j, N); 
    C[ij_idx] = 0;
    for (int k = 0; k < M; ++k) {
        const double& c1 = A[IDX2C(i, k, N)];
        const double& c2 = B[IDX2C(k, j, M)];
        C[ij_idx] += c1 * c2;
    }
}
```

<h3>Системные средства</h3>
Операционная система – Windows 10 (компилятор NVCC).

<h3>Аппаратные средства</h3>


Центральный процессор – AMD Ryzen 5 3600. Количество ядер – 6. Тактовая частота – 3,6 ГГц или 3600 МГц.

Видеокарта – NVIDIA GeForce GTX 1650. Количество CUDA-ядер – 896. Тактовая частота – 1410 МГц.

<h2>Выбор параметров эксперимента</h2>

При параметризации акцентируем три аспекта:
- изменение ускорения с общим ростом размерностей перемножаемых матриц;
- изменение ускорения при росте отдельных размерностей перемножаемых матриц – по строкам или по столбцам;
- изменения, возникающие с использованием FMA.

Перемножаемые матрицы характеризуются размерностями: MxN, NxK. Следовательно, регулировать будем их.

Были выбраны следующие триплеты (M, N, K): (100, 100, 100), (200, 100, 100), (100, 200, 100), (100, 100, 200), (300, 300, 300), (600, 300, 300), (300, 600, 300), (300, 300, 600), (800, 800, 800), (1600, 800, 800), (800, 1600, 800), (800, 800, 1600). 

Соответственно, для выбранных триплетов будет рассмотрено ускорение при примененном FMA и при отсутствующем FMA, что регулируется параметром fmad. 


<h2>Теоретическое предсказание результатов эксперимента</h2>

По закону Амдала можно было бы ожидать ускорение 896, поскольку имеется 896 CUDA-ядер, а в последовательной реализации используется одно ядро процессора. Однако тактовая частота ядра центрального процессора и CUDA-ядер разная. В случае центрального процессора при соответствующем уровне оптимизации возможны также векторизованные вычисления, однако в используемой здесь реализации векторизации поддаются лишь переменные ij_idx и c2, что свидетельствует о том, что векторизация вероятнее всего не будет произведена. 

FMA – multiply-add operation – это операция вида $rn(X⋅Y+Z)$ только с одним шагом округления. Без FMA приходится выполнять два шага округления: $rn(rn(X⋅Y)+Z)$, что, во-первых, дольше, а во-вторых, менее точно. FMA можно включить, задав опцию $fmad=true$.

Поскольку в одном из вариантов параллельной реализации используются FMA-вычисления, а попарное умножение и сложение – это основная операция в алгоритме умножения матриц, можно полагать, что пиковая производительность будет в два раза выше. Введем формулы для $CPU Performance$ и $GPU Performance$:

```
CPU Performance = Number of CPU_Cores × CPU Clock Rate,
GPU Performance (No FMA) = Number of GPU Cores × GPU Clock Rate,
GPU Performance (FMA) = Number of GPU Cores × GPU Clock Rate × 2.
```

Подставляем значения в соответствии с характеристиками и получаем:

```
CPU Performance = 3 600 МГц,
GPU Performance (No FMA) = 896×1410=1 263 360 МГц,
GPU Performance (FMA) = 2 526 720 МГц.
```

Таким образом, можем подсчитать, насколько более производителен GPU относительно CPU:

```
Speedup (No FMA) = 351,
Speedup (FMA) = 702.
```

При малых размерах матрицы мы вряд ли задействуем все CUDA-ядра. Чтобы охарактеризовать количество выполняемых операций на одно ядро (процессора или видеокарты) с учетом тактовой частоты, можно рассмотреть формулу:

![изображение](https://github.com/RaisssHab/SamaraUniversity-HPC-Fall-2023/assets/60664914/84992448-d73f-41a0-ad6e-0dd21119fab3)

 Здесь полагается, что каждому CUDA-ядру планировщиком задач назначается один варп, состоящий из 32 нитей. Основной смысл в том, что нитей может быть не так много, чтобы задействовать все ядра, и тогда следует ограничиться количеством ядер равным количеству варпов. 
Следующий график иллюстрирует отношение объема операций на CPU к объему операций на GPU:

![изображение](https://github.com/RaisssHab/SamaraUniversity-HPC-Fall-2023/assets/60664914/69f4356f-0bef-4cb3-abd0-00e296b6ec6c)

Таким образом, мы уточнили теоретическое ускорение, достигаемое при различных конфигурациях исходных матриц. Видно, что после тройки (300, 300, 300) достигается максимальное ускорение. Ожидается, что ускорение резко снизится при конфигурации (100, 200, 100).

<h2>Проведение эксперимента</h2>

Для проведения эксперимента использовался написанный скрипт из файла run_matmul.bat. В нем задаются тройки параметров матриц, для каждой тройки происходит сначала генерация матрицы с сохранением файл, а затем выполнение программы на CUDA. Вывод программы записывается в файл log.txt. 
Далее представлен отрывок кода:

```
echo fmad=false
echo fmad=false >> %log_file%
rem Loop through each triplet
for /l %%i in (0,1,!end_value!) do (
    set N=!Ns[%%i]!
    set M=!Ms[%%i]!
    set K=!Ks[%%i]!

    rem Run HPC MatMul input data.exe
	echo "HPC MatMul input data.exe" !N! !M! !K!
    "HPC MatMul input data.exe" !N! !M! !K!

    rem Run HPC MatMul.exe and record the output to the log file
    echo N = !N! M = !M! K = !K! combination: >> %log_file%
    "HPC MatMul (fmad=false).exe" >> %log_file%
    echo. >> %log_file%
)
```


<h2>Представление результатов</h2>

Результаты представлены в таблице 1 и на рисунках 1-3.

Таблица 1 – Время алгоритмов и ускорение параллельной реализации

|Последовательное время, мкс|fmad=false, мкс|fmad=true, мкс|fmad=false, ускорение|fmad=true, ускорение|
| :-: | :-: | :-: | :-: | :-: |
|100|13|7|8,0|14,7|
|100|22|10|4,6|9,9|
|100|22|11|4,6|8,9|
|100|22|10|4,6|10,0|
|1800|192|72|9,4|25,1|
|3600|366|186|9,8|19,3|
|3800|376|200|10,1|19,0|
|3600|364|181|9,9|19,9|
|40300|2815|1561|14,3|25,8|
|109900|5179|2822|21,2|38,9|
|89100|5012|2932|17,8|30,4|
|83600|4851|2687|17,2|31,1|

 ![изображение](https://github.com/RaisssHab/SamaraUniversity-HPC-Fall-2023/assets/60664914/4cdb2f60-ad24-45a6-af3a-de5f1358acac)

Рисунок 1 – График ускорения параллельной реализации алгоритма

![изображение](https://github.com/RaisssHab/SamaraUniversity-HPC-Fall-2023/assets/60664914/47af9b5d-353c-46f4-ace6-c7d97bee5211)

Рисунок 2 – График ускорения параллельной реализации алгоритма при M=N=K=100; 300; 800

![изображение](https://github.com/RaisssHab/SamaraUniversity-HPC-Fall-2023/assets/60664914/d9e3b5c1-49e4-42ca-8def-dd8a9551b5e3)

Рисунок 3 – График теоретического и фактического ускорения параллельной реализации алгоритма

<h2>Описание результатов</h2>

1.	Фактические ускорения более, чем на порядок меньше теоретических.
2.	В целом вычисления с использованием FMA действительно в два раза быстрее по сравнению с их отсутствием.
3.	При конфигурации (100, 100, 100) ускорение больше, чем при конфигурациях, где один из параметров вдвое больше.
4.	При конфигурации (100, 200, 100) в случае отсутствия FMA ускорение совсем не падает сравнительно с конфигурациями (200, 100, 100) и (100, 100, 200), а в случае FMA уменьшается, как ожидалось теоретически, но не в два раза, а существенно меньше.
5.	Конфигурация (300, 300, 300) не является точкой, начиная с которой возникает максимальное ускорение.
6.	При конфигурации (300, 300, 300) в случае FMA происходит «скачок» ускорения относительно конфигураций, где один из параметров вдвое выше.
7.	При конфигурации (1600, 800, 800) также происходит «скачок» ускорения относительно конфигураций (800, 800, 800), (800, 1600, 800), (800, 800, 1600).

<h2>Анализ результатов</h2>

1. Можно предположить влияние нескольких факторов: 1) задействованы не все ядра GPU; 2) тактовая частота GPU могла быть ниже; 3) рассматривалась базовая частота CPU, однако, по наблюдениям, она при работе достигает 4,1 ГГц; 4) замедление за счет работы с памятью.
2. Имеем качественное совпадение.
3. Возможно, связано с особенностями планировщика задач, с затратами на организацию параллелизма.
4. Не удалось предложить объяснение.
5. Возможно, что ядра GPU задействуются не так, как предполагалось теоретически, в меньшем количестве, а не максимальном.
6. Не удалось предложить объяснение.
7. Можно объяснить оптимизациями компилятора. Рассмотрим цикл 

```
for (int i = 0; i < N; ++i) {
      for (int j = 0; j < K; ++j) {
          int ij_idx = IDX2C(i, j, N);
          for (int k = 0; k < M; ++k) {
              const double& c1 = A[IDX2C(i, k, N)];
              const double& c2 = B[IDX2C(k, j, M)];
              C[ij_idx] += c1 * c2;
          }
      }
  }
```

IDX2C(i,j,N)=i⋅N+j.

Можно видеть, что IDX2C(i,j,N) для параметра N используется дважды в цикле, причем в сочетании с i. Это позволяет не вычислять операцию умножения каждый раз в двух вложенных циклах, а посчитать один раз и использовать результат далее. В этом отношении выражение относительно N очень удачно подлежит оптимизации, что сложно сказать о параметре M. 

IDX2C(k,j,M) не подлежит оптимизации, поскольку итерируемая переменная k находится в третьем по вложенности цикле, из-за чего произведение приходится вычислять всегда. Это способствует замедлению относительно N при росте M.

В цикле нет выражений IDX2C, зависящих непосредственно от K, но имеется переменная j, которая всегда складывается в циклах второго и третьего уровня вложенности, что может делать увеличение размерности по K менее выгодным, чем по размерности N.

<h2>Заключение</h2>

Таким образом, в ходе данной работы была достигнута поставленная цель – проанализировано ускорение параллельной программы на CUDA. 

Теоретические ожидания совпали только в качестве оценки сверху, а также до тройки (300, 300, 300) можем наблюдать, что сначала ускорение меньше, а потом больше. На количественном уровне совпадений не выявлено. Однако в целом вычисления с использованием FMA действительно примерно в два раза быстрее по сравнению с их отсутствием.

При анализе результатов были сделаны предположения, что ускорение может также зависеть от: 1) планировщика задач; 2) фактической тактовой частоты CPU и GPU; 3) от работы с памятью; 4) от самого алгоритма и соответствующих оптимизаций компилятора. Для части описанных результатов в принципе не удалось предложить объяснение.

Возникли новые вопросы, подлежащие исследованию: 1) каково влияние описанных выше факторов; 2) почему при конфигурации (100, 200, 100) ускорение почти не уменьшилось по сравнению с конфигурациями (200, 100, 100) и (100, 100, 200); 3) почему при конфигурации (300, 300, 300) в случае FMA происходит «скачок» ускорения относительно конфигураций, где один из параметров вдвое выше.
