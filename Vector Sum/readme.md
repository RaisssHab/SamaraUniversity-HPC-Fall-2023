<h1>Vector Sum</h1>
<h2>Постановка цели эксперимента</h2>

Цель – исследовать ускорение при вычислении суммы вектора на CUDA относительно последовательной реализации.

Задачи:
1.	Рассчитать время, достигаемое последовательной реализацией алгоритма.
2.	Рассчитать время и ускорение, достигаемое с использованием написанной параллельной реализации на CUDA.
3.	Проанализировать результат, сделать выводы.

<h2>Инструментальные средства эксперимента</h2>
<h3>Программные средства</h3>
Язык программирования – C++. 

Последовательная реализация:

```
double vectorSumCPU(double *input, int size) {
    double sum = 0;
    for (int i = 0; i < size; ++i) {
        sum += input[i];
    }
    return sum;
}
```

Поэлементный проход с суммированием в переменную.

Параллельные реализации:

Были взяты на основе https://developer.download.nvidia.com/compute/cuda/1.1-Beta/x86_website/projects/reduction/doc/reduction.pdf, слайды 7 и 11.

![изображение](https://github.com/RaisssHab/SamaraUniversity-HPC-Fall-2023/assets/60664914/047691f3-4ac0-47ff-aeb3-d79dab4cb3d4)

```
__global__ void vectorSumShared(double* input, double* output, int size) {
    extern __shared__ double sdata[];
    // each thread loads one element from global to shared mem
    unsigned int tid = threadIdx.x;
    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < size) {
        sdata[tid] = input[i];
    }
    else {
        sdata[tid] = 0;
        return;
    }
    __syncthreads();
    
    // do reduction in shared mem
    for (unsigned int s = 1; s < blockDim.x; s *= 2) {
        int index = 2 * s * tid;
        if (index + s < blockDim.x) {
            sdata[index] += sdata[index + s];
        }
        __syncthreads();
    }
    // write result for this block to global mem
    if (tid == 0) output[blockIdx.x] = sdata[0];
    //output[i] = sdata[tid];
}
```

Суть реализации - выполнить подсчет в рамках блока, записать результат в начало выходного массива (который может являться входом). Сначала происходит сложение с соседом на расстоянии 1, затем на расстоянии 2 и так далее. При этом нити растягиваются по блоку на каждом новом витке итерации, и таким образом все больше нитей перестают быть задействованными в вычислениях. На каждом витке складываются уже результаты не исходных значений, а результат промежуточной агрегации предыдущего уровня.

Проблема реализации - bank conflicts, поскольку shared-память в блоке поделена на участки по 32 элемента (на которых производят вычисления варпы), и при одновременной записи, например, в 0-й элемент и 32-й элемент shared-памяти будет произведена сериализация, т.е. последовательное выполнение операций записи, вместо параллельного. Поэтому следующее улучшение направлено на ликвидацию этого недостатка. Однако в данной работе мы остановимся на текущей реализации.

Предложенная реализация адаптирована и под глобальную память. Если в случае shared-памяти не приходилось думать о сдвиге относительно выходного массива, то для реализации с глобальной памятью такая необходимость возникает. Используется blockStart, чтобы определить индекс в выходном массиве для начала блока.

```
for (unsigned int s = 1; s < blockDim.x; s *= 2) {
    int index = 2 * s * i;
    if ((index + s < blockDim.x) && (blockStart + index + s < size)) {
        output[blockStart + index] += output[blockStart + index + s];
    }
    __syncthreads();
}
```

Поскольку ядро не является полностью самостоятельной функцией, нужно вызывать его несколько раз для нескольких уровней агрегации. Это выполнено следующим образом:

```
void launchVectorSumShared(double* input, int size) {
    while (size > 1) {
        int gridSize = (size + 1023) / 1024;
        vectorSumShared << < gridSize, 1024, 1024 * sizeof(double) >> > (input, input, size);
        cudaError_t cuerr = cudaGetLastError();
        if (cuerr != cudaSuccess)
        {
            fprintf(stderr, "Cannot launch CUDA kernel: %s\n",
                cudaGetErrorString(cuerr));
            return;
        }

        // синхронизация устройств
        SAFE_CALL(cudaDeviceSynchronize(),
            "Cannot synchronize CUDA kernel: %s\n");
        size = gridSize;
    }
}
```

Здесь вы устанавливаем размер блока в 1024 для каждого вызова ядра. Размер массива каждый раз уменьшается в 1024 раза.

Из-за изначальных проблем с реализацией был ошибочно сделан вывод, что shared-память работает на очень малом объеме данных, не позволяя вместить весь массив. Это не так. Но была предложена реализация с порционным вычислением на основе shared-памяти, которую далее также исследуем. Реализации две - с большими коммуникациями с CPU и с отсутствием коммуникаций.

Без коммуникаций с CPU код следующий:

```
void launchVectorSumSharedPortionalNoCPU(double* input, int size) {
    int portion_size = 1000000;
    int chunks = (size + portion_size - 1) / portion_size;

    int cur_chunk = 0;
    for (int chunk = 0; chunk < chunks; ++chunk) {
        int i = chunk * portion_size;
        double* intermediate_input = input + i;
        int intermediate_size = ((size - i) > portion_size) ? portion_size : (size - i);
        //std::cout << intermediate_size << "\n";
        launchVectorSumShared(intermediate_input, intermediate_size);
        SAFE_CALL(cudaMemcpy(input + chunk, intermediate_input, sizeof(double), cudaMemcpyDeviceToDevice),
            "Cannot copy C array from device to host: %s\n");
    }
    if (chunks > 1) {
        launchVectorSumSharedPortionalNoCPU(input, chunks);
    }
}
```

Копирование остается, но внутри GPU. Это нужно для составления очередного промежуточного массива для агрегации, который далее отправляется рекуррентно в эту функцию.

<h3>Системные средства</h3>
Операционная система – Windows 10 (компилятор NVCC).

<h3>Аппаратные средства</h3>


Центральный процессор – AMD Ryzen 5 3600. Количество ядер – 6. Тактовая частота – 3,6 ГГц или 3600 МГц.

Видеокарта – NVIDIA GeForce GTX 1650. Количество CUDA-ядер – 896. Тактовая частота – 1410 МГц.

<h2>Выбор параметров эксперимента</h2>

Будет проведено два эксперимента: 1) со сравнением работы ядра с глоабльной памятью и с shared-памятью на размерах массива 50000, 200000, 1000000; 2) со сравнением реализаций порционного вычисления на размерах массива 25000, 50000, 200000, 1000000. Выбор обоснован во втором случае тем, что предварительно отмечено, что на размере 50000 удается достичь ускорения, примерно равного 1.

<h2>Теоретическое предсказание результатов эксперимента</h2>

1) реализация с shared-памятью даст ускорение выше, чем реализация с глобальной памятью;
2) до определенного чанка ускорение будет меньше 1, однако после определенного размера ускорение начнет расти и приближаться к тому, что наблюдается в первом эксперименте.

<h2>Проведение эксперимента</h2>

Для разных параметров в Visual Studio 2022 запускается код, в котором уже выполнены все необходимые вычисления и получено ускорение. Эти значения выписываются и далее строятся графики.


<h2>Представление результатов</h2>

Результаты представлены рисунках 1-2.

![изображение](https://github.com/RaisssHab/SamaraUniversity-HPC-Fall-2023/assets/60664914/867b019f-e9ef-4f97-acd8-534585cd5309)

Рисунок 1 – Графики ускорения параллельных реализаций, сравнение глобальной памяти с shared-памятью

![изображение](https://github.com/RaisssHab/SamaraUniversity-HPC-Fall-2023/assets/60664914/cc7018e0-f4fd-4027-9199-f0b3c56b709f)

Рисунок 2 – Графики ускорения параллельных реализаций, сравнение реализаций на shared-памяти с чанками

<h2>Описание результатов</h2>

1.	Ускорение на основе shared-памяти выше, чем на глобальной, но не существенно.
2.	Реализация с чанками с коммуникациями внутри GPU лучше, чем с коммуникациями с CPU.

<h2>Анализ результатов</h2>

1. Доступ к shared-памяти существенно быстрее, чем к глобальной. Несущественность ускорения может быть вызывана недостаточно большим размером массива, недочетами в выборе параметров ядра, недостаточно хорошей реализации, оценка в debug-режиме.
2. Обмен данными между CPU и GPU дольше, чем внутри GPU.

<h2>Заключение</h2>

Таким образом, в ходе данной работы была достигнута поставленная цель – проанализировано ускорение параллельной программы на CUDA. 

Теоретические ожидания совпали c практическими результатами, оданко текущая реализация на shared-памяти не сильно превосходит реализацию на глобальной памяти.
